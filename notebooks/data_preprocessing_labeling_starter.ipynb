{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import re\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='../logs/data_processing.log',\n",
    "                    level=logging.INFO,\n",
    "                    format='%(asctime)s:%(levelname)s:%(message)s')\n",
    "  \n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../scripts')))\n",
    "from load_data import Load_Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('../data/telegram_data.xlsx')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for NaN values in the 'Message' column:\n",
      "Number of NaN values in 'Message' column: 1849\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking for NaN values in the 'Message' column:\")\n",
    "nan_count = df['Message'].isnull().sum()\n",
    "print(f\"Number of NaN values in 'Message' column: {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape after dropping NaN values in 'Message' column: (3166, 6)\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna(subset=['Message'])\n",
    "\n",
    "# Print the shape of the dataset after dropping NaN values in the \"Message\" column\n",
    "print(f\"Dataset shape after dropping NaN values in 'Message' column: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel Title</th>\n",
       "      <th>Channel Username</th>\n",
       "      <th>ID</th>\n",
       "      <th>Message</th>\n",
       "      <th>Date</th>\n",
       "      <th>Media Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sheger online-store</td>\n",
       "      <td>@Shageronlinestore</td>\n",
       "      <td>5328.0</td>\n",
       "      <td>ğŸ’¥3pcs silicon brush spatulas\\n\\n\\nÂ Â Â Â Â  \\nÂ Â Â Â ...</td>\n",
       "      <td>2024-09-20 11:50:02+00:00</td>\n",
       "      <td>photos/@Shageronlinestore_5328.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sheger online-store</td>\n",
       "      <td>@Shageronlinestore</td>\n",
       "      <td>5327.0</td>\n",
       "      <td>ğŸ’¥Mandoline Slicer\\n\\nğŸ‘‰ áŒŠá‹œ á‰†áŒ£á‰¢ áˆµáˆ‹á‹­áˆµ áˆ›á‹µáˆ¨áŒŠá‹« \\nğŸ‘‰Â  ...</td>\n",
       "      <td>2024-09-20 08:11:40+00:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sheger online-store</td>\n",
       "      <td>@Shageronlinestore</td>\n",
       "      <td>5326.0</td>\n",
       "      <td>ğŸ’¥Table Desk Edge Guard Strip\\nÂ Â Â Â Â Â  ğŸ’¯ High Qu...</td>\n",
       "      <td>2024-09-20 05:23:18+00:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sheger online-store</td>\n",
       "      <td>@Shageronlinestore</td>\n",
       "      <td>5325.0</td>\n",
       "      <td>ğŸ’¥Table Desk Edge Guard Strip\\nÂ Â Â Â Â Â  ğŸ’¯ High Qu...</td>\n",
       "      <td>2024-09-20 05:21:14+00:00</td>\n",
       "      <td>photos/@Shageronlinestore_5325.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sheger online-store</td>\n",
       "      <td>@Shageronlinestore</td>\n",
       "      <td>5323.0</td>\n",
       "      <td>ğŸ’¥Only baby 3in1 double bottle milk warmer,ster...</td>\n",
       "      <td>2024-09-19 13:54:46+00:00</td>\n",
       "      <td>photos/@Shageronlinestore_5323.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Channel Title    Channel Username      ID  \\\n",
       "5   Sheger online-store  @Shageronlinestore  5328.0   \n",
       "6   Sheger online-store  @Shageronlinestore  5327.0   \n",
       "7   Sheger online-store  @Shageronlinestore  5326.0   \n",
       "8   Sheger online-store  @Shageronlinestore  5325.0   \n",
       "10  Sheger online-store  @Shageronlinestore  5323.0   \n",
       "\n",
       "                                              Message  \\\n",
       "5   ğŸ’¥3pcs silicon brush spatulas\\n\\n\\nÂ Â Â Â Â  \\nÂ Â Â Â ...   \n",
       "6   ğŸ’¥Mandoline Slicer\\n\\nğŸ‘‰ áŒŠá‹œ á‰†áŒ£á‰¢ áˆµáˆ‹á‹­áˆµ áˆ›á‹µáˆ¨áŒŠá‹« \\nğŸ‘‰Â  ...   \n",
       "7   ğŸ’¥Table Desk Edge Guard Strip\\nÂ Â Â Â Â Â  ğŸ’¯ High Qu...   \n",
       "8   ğŸ’¥Table Desk Edge Guard Strip\\nÂ Â Â Â Â Â  ğŸ’¯ High Qu...   \n",
       "10  ğŸ’¥Only baby 3in1 double bottle milk warmer,ster...   \n",
       "\n",
       "                         Date                          Media Path  \n",
       "5   2024-09-20 11:50:02+00:00  photos/@Shageronlinestore_5328.jpg  \n",
       "6   2024-09-20 08:11:40+00:00                                 NaN  \n",
       "7   2024-09-20 05:23:18+00:00                                 NaN  \n",
       "8   2024-09-20 05:21:14+00:00  photos/@Shageronlinestore_5325.jpg  \n",
       "10  2024-09-19 13:54:46+00:00  photos/@Shageronlinestore_5323.jpg  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5       ğŸ’¥3pcs silicon brush spatulas\\n\\n\\nÂ Â Â Â Â  \\nÂ Â Â Â ...\n",
       "6       ğŸ’¥Mandoline Slicer\\n\\nğŸ‘‰ áŒŠá‹œ á‰†áŒ£á‰¢ áˆµáˆ‹á‹­áˆµ áˆ›á‹µáˆ¨áŒŠá‹« \\nğŸ‘‰Â  ...\n",
       "7       ğŸ’¥Table Desk Edge Guard Strip\\nÂ Â Â Â Â Â  ğŸ’¯ High Qu...\n",
       "8       ğŸ’¥Table Desk Edge Guard Strip\\nÂ Â Â Â Â Â  ğŸ’¯ High Qu...\n",
       "10      ğŸ’¥Only baby 3in1 double bottle milk warmer,ster...\n",
       "                              ...                        \n",
       "5009    ğŸ¯ Kitchen Sticker\\n\\náˆˆáŠªá‰½áŠ•á‹ á‹á‰ á‰µ áŠ¥áŒ…áŒ á‰°áˆ˜áˆ«áŒ­ \\nğŸ”°á‹áˆ€ ...\n",
       "5010    ğŸ¯ 3in1 One Step Hair Dryer & Styler \\n\\nğŸ‘‰ áŠ¨áˆ­áˆ ...\n",
       "5011    âœ… Home GYM - X5 slimming vibrator \\n\\nğŸ“¢ğŸ“¢ğŸ“¢ á‰³áˆ‹á‰… ...\n",
       "5012    áˆˆáŒ¤áŠ“á‰½áŠ•-Health & Personal Care\\n\\nğŸ“FingerTip Pul...\n",
       "5013    #Finger_tip_pulse_oximeter\\n       #á‰ á‰°áˆ˜áŒ£áŒ£áŠ_á‹‹áŒ‹\\...\n",
       "Name: Message, Length: 3166, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_df=df['Message']\n",
    "message_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Channel Title    Channel Username      ID  \\\n",
      "5   Sheger online-store  @Shageronlinestore  5328.0   \n",
      "6   Sheger online-store  @Shageronlinestore  5327.0   \n",
      "7   Sheger online-store  @Shageronlinestore  5326.0   \n",
      "8   Sheger online-store  @Shageronlinestore  5325.0   \n",
      "10  Sheger online-store  @Shageronlinestore  5323.0   \n",
      "\n",
      "                                              Message  \\\n",
      "5   3pcs silicon brush spatulas\\n\\n\\nÂ Â Â Â Â  \\nÂ Â Â Â Â ...   \n",
      "6   Mandoline Slicer\\n\\n áŒŠá‹œ á‰†áŒ£á‰¢ áˆµáˆ‹á‹­áˆµ áˆ›á‹µáˆ¨áŒŠá‹« \\nÂ  áˆˆáŠ¥áŒ…...   \n",
      "7   Table Desk Edge Guard Strip\\nÂ Â Â Â Â Â   High Qual...   \n",
      "8   Table Desk Edge Guard Strip\\nÂ Â Â Â Â Â   High Qual...   \n",
      "10  Only baby 3in1 double bottle milk warmer,steri...   \n",
      "\n",
      "                         Date                          Media Path  \n",
      "5   2024-09-20 11:50:02+00:00  photos/@Shageronlinestore_5328.jpg  \n",
      "6   2024-09-20 08:11:40+00:00                                 NaN  \n",
      "7   2024-09-20 05:23:18+00:00                                 NaN  \n",
      "8   2024-09-20 05:21:14+00:00  photos/@Shageronlinestore_5325.jpg  \n",
      "10  2024-09-19 13:54:46+00:00  photos/@Shageronlinestore_5323.jpg  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example of your DataFrame\n",
    "# df = pd.DataFrame({'Message': ['ğŸ’¥3pcs silicon brush spatulas...', 'ğŸ’¥Mandoline Slicer...', ...]})\n",
    "\n",
    "# Define a function to remove emojis\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\" \n",
    "        \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "        \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "        \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "        \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "        \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "        \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "        \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "        \"\\U000024C2-\\U0001F251\" \n",
    "        \"]+\", \n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "# Apply the function to the 'Message' column\n",
    "df['Message'] = df['Message'].apply(remove_emojis)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.to_csv('../data/clean_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Labeling for Product, Price, and Location Recognition in UTF-8 Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel Title</th>\n",
       "      <th>Channel Username</th>\n",
       "      <th>ID</th>\n",
       "      <th>Message</th>\n",
       "      <th>Date</th>\n",
       "      <th>Media Path</th>\n",
       "      <th>Labeled_Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sheger online-store</td>\n",
       "      <td>@Shageronlinestore</td>\n",
       "      <td>5328.0</td>\n",
       "      <td>3pcs silicon brush spatulas\\n\\n\\nÂ Â Â Â Â  \\nÂ Â Â Â Â ...</td>\n",
       "      <td>2024-09-20 11:50:02+00:00</td>\n",
       "      <td>photos/@Shageronlinestore_5328.jpg</td>\n",
       "      <td>3pcs B-PRODUCT\\nsilicon I-PRODUCT\\nbrush I-PRO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sheger online-store</td>\n",
       "      <td>@Shageronlinestore</td>\n",
       "      <td>5327.0</td>\n",
       "      <td>Mandoline Slicer\\n\\n áŒŠá‹œ á‰†áŒ£á‰¢ áˆµáˆ‹á‹­áˆµ áˆ›á‹µáˆ¨áŒŠá‹« \\nÂ  áˆˆáŠ¥áŒ…...</td>\n",
       "      <td>2024-09-20 08:11:40+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mandoline B-PRODUCT\\nSlicer I-PRODUCT\\náŒŠá‹œ O\\ná‰†...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sheger online-store</td>\n",
       "      <td>@Shageronlinestore</td>\n",
       "      <td>5326.0</td>\n",
       "      <td>Table Desk Edge Guard Strip\\nÂ Â Â Â Â Â   High Qual...</td>\n",
       "      <td>2024-09-20 05:23:18+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Table B-PRODUCT\\nDesk I-PRODUCT\\nEdge I-PRODUC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sheger online-store</td>\n",
       "      <td>@Shageronlinestore</td>\n",
       "      <td>5325.0</td>\n",
       "      <td>Table Desk Edge Guard Strip\\nÂ Â Â Â Â Â   High Qual...</td>\n",
       "      <td>2024-09-20 05:21:14+00:00</td>\n",
       "      <td>photos/@Shageronlinestore_5325.jpg</td>\n",
       "      <td>Table B-PRODUCT\\nDesk I-PRODUCT\\nEdge I-PRODUC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sheger online-store</td>\n",
       "      <td>@Shageronlinestore</td>\n",
       "      <td>5323.0</td>\n",
       "      <td>Only baby 3in1 double bottle milk warmer,steri...</td>\n",
       "      <td>2024-09-19 13:54:46+00:00</td>\n",
       "      <td>photos/@Shageronlinestore_5323.jpg</td>\n",
       "      <td>Only B-PRODUCT\\nbaby I-PRODUCT\\n3in1 I-PRODUCT...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Channel Title    Channel Username      ID  \\\n",
       "5   Sheger online-store  @Shageronlinestore  5328.0   \n",
       "6   Sheger online-store  @Shageronlinestore  5327.0   \n",
       "7   Sheger online-store  @Shageronlinestore  5326.0   \n",
       "8   Sheger online-store  @Shageronlinestore  5325.0   \n",
       "10  Sheger online-store  @Shageronlinestore  5323.0   \n",
       "\n",
       "                                              Message  \\\n",
       "5   3pcs silicon brush spatulas\\n\\n\\nÂ Â Â Â Â  \\nÂ Â Â Â Â ...   \n",
       "6   Mandoline Slicer\\n\\n áŒŠá‹œ á‰†áŒ£á‰¢ áˆµáˆ‹á‹­áˆµ áˆ›á‹µáˆ¨áŒŠá‹« \\nÂ  áˆˆáŠ¥áŒ…...   \n",
       "7   Table Desk Edge Guard Strip\\nÂ Â Â Â Â Â   High Qual...   \n",
       "8   Table Desk Edge Guard Strip\\nÂ Â Â Â Â Â   High Qual...   \n",
       "10  Only baby 3in1 double bottle milk warmer,steri...   \n",
       "\n",
       "                         Date                          Media Path  \\\n",
       "5   2024-09-20 11:50:02+00:00  photos/@Shageronlinestore_5328.jpg   \n",
       "6   2024-09-20 08:11:40+00:00                                 NaN   \n",
       "7   2024-09-20 05:23:18+00:00                                 NaN   \n",
       "8   2024-09-20 05:21:14+00:00  photos/@Shageronlinestore_5325.jpg   \n",
       "10  2024-09-19 13:54:46+00:00  photos/@Shageronlinestore_5323.jpg   \n",
       "\n",
       "                                      Labeled_Message  \n",
       "5   3pcs B-PRODUCT\\nsilicon I-PRODUCT\\nbrush I-PRO...  \n",
       "6   Mandoline B-PRODUCT\\nSlicer I-PRODUCT\\náŒŠá‹œ O\\ná‰†...  \n",
       "7   Table B-PRODUCT\\nDesk I-PRODUCT\\nEdge I-PRODUC...  \n",
       "8   Table B-PRODUCT\\nDesk I-PRODUCT\\nEdge I-PRODUC...  \n",
       "10  Only B-PRODUCT\\nbaby I-PRODUCT\\n3in1 I-PRODUCT...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def label_message_utf8_with_birr(message):\n",
    "    # Check if the message is None or empty\n",
    "    if not isinstance(message, str) or message.strip() == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    # Split the message at the first occurrence of '\\n'\n",
    "    if '\\n' in message:\n",
    "        first_line, remaining_message = message.split('\\n', 1)\n",
    "    else:\n",
    "        first_line, remaining_message = message, \"\"\n",
    "    \n",
    "    labeled_tokens = []\n",
    "    \n",
    "    # Tokenize the first line\n",
    "    first_line_tokens = re.findall(r'\\S+', first_line)\n",
    "    \n",
    "    # Label the first token as B-PRODUCT and the rest as I-PRODUCT\n",
    "    if first_line_tokens:\n",
    "        labeled_tokens.append(f\"{first_line_tokens[0]} B-PRODUCT\")  # First token as B-PRODUCT\n",
    "        for token in first_line_tokens[1:]:\n",
    "            labeled_tokens.append(f\"{token} I-PRODUCT\")  # Remaining tokens as I-PRODUCT\n",
    "    \n",
    "    # Process the remaining message normally\n",
    "    if remaining_message:\n",
    "        lines = remaining_message.split('\\n')\n",
    "        for line in lines:\n",
    "            tokens = re.findall(r'\\S+', line)  # Tokenize each line\n",
    "            \n",
    "            for token in tokens:\n",
    "                # Check if token is a price (e.g., 500 ETB, $100, or á‰¥áˆ­)\n",
    "                if re.match(r'^\\d{10,}$', token):\n",
    "                    labeled_tokens.append(f\"{token} O\")  # Label as O for \"other\" or outside of any entity\n",
    "                elif re.match(r'^\\d+(\\.\\d{1,2})?$', token) or 'ETB' in token or 'á‹‹áŒ‹' in token or '$' in token or 'á‰¥áˆ­' in token:\n",
    "                    labeled_tokens.append(f\"{token} I-PRICE\")\n",
    "                # Check if token could be a location (e.g., cities or general location names)\n",
    "                elif any(loc in token for loc in ['Addis Ababa', 'áˆˆá‰¡', 'áˆˆá‰¡Â áˆ˜á‹³áˆ…áŠ’á‹“áˆˆáˆ', 'áˆ˜áŒˆáŠ“áŠ›', 'á‰¦áˆŒ', 'áˆœáŠ­áˆ²áŠ®']):\n",
    "                    labeled_tokens.append(f\"{token} I-LOC\")\n",
    "                # Assume other tokens are part of a product name or general text\n",
    "                else:\n",
    "                    labeled_tokens.append(f\"{token} O\")\n",
    "    \n",
    "    return \"\\n\".join(labeled_tokens)\n",
    "\n",
    "# Apply the updated function to the non-null messages\n",
    "df['Labeled_Message'] = df['Message'].apply(lambda x: label_message_utf8_with_birr(x) if x is not None else \"\")\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated labeled dataset to a file in CoNLL format\n",
    "labeled_data_birr_path = 'labeled_telegram_product_price_location.txt-'\n",
    "with open(labeled_data_birr_path, 'w', encoding='utf-8') as f:\n",
    "    for index, row in df.iterrows():\n",
    "        f.write(f\"{row['Labeled_Message']}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories = {\n",
    "#     'kids': [\n",
    "#         'toy', 'children', 'kids', 'áˆ˜áŒ«á‹ˆá‰»', 'play', 'games', 'fun', 'educational', \n",
    "#         'puzzle', 'doll', 'action figure', 'stuffed animal', 'arts and crafts', \n",
    "#         'books', 'outdoor toys', 'building blocks', 'baby', 'toddler', 'Baby',\n",
    "#         'áˆ˜áŒ«á‹ˆá‰»á‹á‰½'\n",
    "#     ],\n",
    "#     'men': [\n",
    "#         'men', 'grooming', 'shaving', 'beard', 'razor', 'aftershave', \n",
    "#         'scent', 'deodorant', 'grooming kit', 'haircut', 'fashion', 'suits', \n",
    "#         'wallet', 'watch', 'accessories', 'fitness', 'shoes', \n",
    "#         'áŠ áˆµá‰°áŠ«áŠ­áˆ', 'á‹¨á‰¥áˆ­áˆƒáŠ• á‹•á‰ƒá‹á‰½'\n",
    "#     ],\n",
    "#     'women': [\n",
    "#         'women', 'makeup', 'hair dryer', 'lipstick', 'foundation', 'mascara', \n",
    "#         'skincare', 'nails', 'jewelry', 'dresses', 'handbags', 'accessories', \n",
    "#         'fashion', 'shoes', 'perfume', 'hairstyle', 'wellness', 'beauty', 'style','Hair Drye',\n",
    "#         'áŠ¥áŠ•á‰…áˆµá‰ƒáˆ´', 'á‹¨á€áŒ‰áˆ­ áŠ¥á‰ƒá‹á‰½', 'á‹¨á‹á‰ á‰µ áŠ¥á‰ƒá‹á‰½'\n",
    "#     ],\n",
    "#     'sport': [\n",
    "#         'gym', 'GYM','fitness', 'exercise', 'áŠ¥áŠ•á‰…áˆµá‰ƒáˆ´', 'workout', 'training', 'yoga', \n",
    "#         'running', 'cycling', 'sportswear', 'equipment', 'weights', 'cardio', \n",
    "#         'aerobics', 'team sports', 'outdoor activities', 'athletics', 'health',  'workout', 'sports',\n",
    "#         'áˆµá–áˆ­á‰µ', 'á‹¨áŠ¥áŠ•á‰…áˆµá‰ƒáˆ´ áˆ˜áˆ³áˆªá‹«á‹á‰½'\n",
    "#     ],\n",
    "#     'groceries': [\n",
    "#         'food', 'snacks', 'grocery', 'áˆáŒá‰¥', 'produce', 'fruits', 'vegetables', \n",
    "#         'meat', 'dairy', 'bread', 'cereal', 'beverages', 'frozen', 'canned', \n",
    "#         'organic', 'bulk', 'condiments', 'spices', 'snack bars', 'breakfast', \n",
    "#         'áŠ¥áŠ•á‰áˆ‹áˆ', 'á‹ˆá‰°áˆ­', 'á‹¨áˆáŒá‰¥ áŠ¥á‰ƒá‹á‰½'\n",
    "#     ],\n",
    "#     'accessories': [\n",
    "#         'jewelry', 'bags', 'accessory', 'á‰€áˆˆá‰ á‰µ', 'belts', 'hats', 'scarves', \n",
    "#         'sunglasses', 'watches', 'hair accessories', 'wallets', 'phone cases', \n",
    "#         'keychains', 'pins', 'brooches', 'fashion', 'style', 'gifts', 'decor', 'á‹¨áˆá‰¥áˆµ áˆ˜á‰¶áŠ¨áˆ»\\n\\n',\n",
    "#         'á‹¨áˆ˜áˆáŠ­á‹• áŠ¥á‰ƒá‹á‰½', 'á‹¨áˆá‰³á‹ˆá‰… áŠ¥á‰ƒá‹á‰½','Anti-theft ',' Earbuds','PowerBank','Grip Tape','humidifier'\n",
    "#     ],\n",
    "#     'health': [\n",
    "#         'health', 'áŒ¤áŠ“', 'wellness', 'nutrition', 'vitamins', 'supplements', \n",
    "#         'exercise', 'fitness', 'mental health', 'meditation', 'stress relief', \n",
    "#         'doctor', 'check-up', 'first aid', 'hygiene', 'immune system', 'balance', \n",
    "#         'self-care', 'áŠ áŠ•á‹°áŠ› áŒ¤áŠ“', 'á‹¨áŒ¤áŠ“ áŠ¥á‰ƒá‹á‰½','pulse'\n",
    "#     ],\n",
    "#     'household': [\n",
    "#         'cleaning', 'furniture', 'decor', 'appliances', 'utensils', 'kitchen', \n",
    "#         'bathroom', 'laundry', 'storage', 'organization', 'home improvement', 'pan', \n",
    "#         'gardening', 'tools', 'supplies', 'safety', 'maintenance', 'pets', 'spatulas','Kitchen','Mop',\n",
    "#         'spatulas\\n\\n','náˆˆáŠªá‰½áŠ•á‹','home', 'comfort', 'á‰¤á‰µ', 'á‹¨á‰¤á‰µ áŠ¥á‰ƒá‹á‰½', 'áŠ¥áŠ•á‰…áˆµá‰ƒáˆ´','bottle','á”áˆ­áˆ™áˆµ','knife',\n",
    "#         'Glass','á‹¨áˆ‹á‹›áŠ›','stove','Ironing Board','Slicer','BLENDER','MULTIFUNCTIONAL BLENDER','Toilet Brush',\n",
    "#         'á‹¨á‰¢áˆ‹ áˆµá‰¥áˆµá‰¥','á‰¢áˆ‹','Oven','fridge', 'áˆ˜áŒ¥á‰ áˆ»','Toilet','Mob','cookware','Blender','KITCHENWARE','áˆáŠ•áŒ£á','Tablemats'\n",
    "#     ]\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_message_utf8_with_birr(message):\n",
    "    tokens = re.findall(r'\\S+', message)  # Tokenize while considering non-ASCII characters\n",
    "    labeled_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        # Check if token is a price (e.g., 500 ETB, $100, or á‰¥áˆ­)\n",
    "        \n",
    "        if re.match(r'^\\d{10,}$', token):\n",
    "            labeled_tokens.append(f\"{token} O\")  # Label as O for \"other\" or outside of any entity\n",
    "        elif re.match(r'^\\d+(\\.\\d{1,2})?$', token) or 'ETB' in token or '$' in token or 'á‰¥áˆ­' in token:\n",
    "            labeled_tokens.append(f\"{token} I-PRICE\")\n",
    "        \n",
    "        # Check if token could be a location (e.g., cities or general location names)\n",
    "        elif any(loc in token for loc in ['Addis Ababa', 'áˆˆá‰¡', 'áˆˆá‰¡Â  áˆ˜á‹³áˆ…áŠ’á‹“áˆˆáˆ', 'áˆ˜áŒˆáŠ“áŠ›','á‰¦áˆŒ','áˆœáŠ­áˆ²áŠ®']):\n",
    "            labeled_tokens.append(f\"{token} I-LOC\")\n",
    "        \n",
    "        elif any(loc in token for loc in ['ğŸ’¥']):\n",
    "            labeled_tokens.append(f\"{token} B-Product\")\n",
    "        \n",
    "        # Assume other tokens are part of a product name (this can be refined)\n",
    "        else:\n",
    "            labeled_tokens.append(f\"{token} O\")\n",
    "    \n",
    "    return \"\\n\".join(labeled_tokens)\n",
    "\n",
    "# Apply the updated function to the non-null messages\n",
    "df['Labeled_Message'] = df['Message'].apply(label_message_utf8_with_birr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated labeled dataset to a file in CoNLL format\n",
    "labeled_data_birr_path = 'labeled_telegram_data_price_product_location_birr.txt'\n",
    "with open(labeled_data_birr_path, 'w', encoding='utf-8') as f:\n",
    "    for index, row in df.iterrows():\n",
    "        f.write(f\"{row['Labeled_Message']}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf.co/repos/fa/39/fa39c3e7e2b904627c4b398a1d015cbbe87073c00877f07ebfe52832984dd391/cfc8146abe2a0488e9e2a0c56de7952f7c11ab059eca145a0a727afce0db2865?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27sentencepiece.bpe.model%3B+filename%3D%22sentencepiece.bpe.model%22%3B&Expires=1727803536&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyNzgwMzUzNn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9mYS8zOS9mYTM5YzNlN2UyYjkwNDYyN2M0YjM5OGExZDAxNWNiYmU4NzA3M2MwMDg3N2YwN2ViZmU1MjgzMjk4NGRkMzkxL2NmYzgxNDZhYmUyYTA0ODhlOWUyYTBjNTZkZTc5NTJmN2MxMWFiMDU5ZWNhMTQ1YTBhNzI3YWZjZTBkYjI4NjU%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=MMBmY25%7EUDV5tyCxEBk-HFgvYzwv3eSLOgFUrL6hPhxoykPcUUdUl%7El1Cg%7E%7Etu7EN26WkLtxpqvqgKWL2n0i51bFb7sFdPLqeah%7EkOmZEYXisGDgApy-1HZTglBFegzirNGWnbmhZt4OSXrlUmWdIFrfOp9jbs9zjodBThHVFhX-FpVqhsaTaZFZlRiEVV3NAV0IFSIUQLiYee1F6IwtM1TqEdsOi1hRgkIz42V464L8iQw-M%7EvYd2g7zUKkPmndg5bdwZZhcEC9OhiVabXgfaIZOmp8evNJP7wysWKTCtWZe1jAy5dTJ%7EvUJD%7EUuOfJOFQaC4ZBzNbHHkemQ8K7cA__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\n requires the protobuf library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\wubeshet.abera\\Projects\\Ten-Academy\\Telegram-EthioMart-Entity-Extraction\\venv\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1452\u001b[0m, in \u001b[0;36mTikTokenConverter.extract_vocab_merges_from_model\u001b[1;34m(self, tiktoken_url)\u001b[0m\n\u001b[0;32m   1451\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1452\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtiktoken\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_tiktoken_bpe\n\u001b[0;32m   1453\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tiktoken'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\wubeshet.abera\\Projects\\Ten-Academy\\Telegram-EthioMart-Entity-Extraction\\venv\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1592\u001b[0m, in \u001b[0;36mconvert_slow_tokenizer\u001b[1;34m(transformer_tokenizer, from_tiktoken)\u001b[0m\n\u001b[0;32m   1588\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting from Tiktoken\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1589\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTikTokenConverter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1591\u001b[0m \u001b[43m        \u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 1592\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1593\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\wubeshet.abera\\Projects\\Ten-Academy\\Telegram-EthioMart-Entity-Extraction\\venv\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1489\u001b[0m, in \u001b[0;36mTikTokenConverter.converted\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1488\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconverted\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tokenizer:\n\u001b[1;32m-> 1489\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1490\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mpre_tokenizer \u001b[38;5;241m=\u001b[39m pre_tokenizers\u001b[38;5;241m.\u001b[39mSequence(\n\u001b[0;32m   1491\u001b[0m         [\n\u001b[0;32m   1492\u001b[0m             pre_tokenizers\u001b[38;5;241m.\u001b[39mSplit(Regex(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpattern), behavior\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124misolated\u001b[39m\u001b[38;5;124m\"\u001b[39m, invert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1493\u001b[0m             pre_tokenizers\u001b[38;5;241m.\u001b[39mByteLevel(add_prefix_space\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_prefix_space, use_regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1494\u001b[0m         ]\n\u001b[0;32m   1495\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\wubeshet.abera\\Projects\\Ten-Academy\\Telegram-EthioMart-Entity-Extraction\\venv\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1482\u001b[0m, in \u001b[0;36mTikTokenConverter.tokenizer\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1481\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenizer\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m-> 1482\u001b[0m     vocab_scores, merges \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_vocab_merges_from_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1483\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(BPE(vocab_scores, merges, fuse_unk\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[1;32mc:\\Users\\wubeshet.abera\\Projects\\Ten-Academy\\Telegram-EthioMart-Entity-Extraction\\venv\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1454\u001b[0m, in \u001b[0;36mTikTokenConverter.extract_vocab_merges_from_model\u001b[1;34m(self, tiktoken_url)\u001b[0m\n\u001b[0;32m   1453\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m-> 1454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1455\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tiktoken` is required to read a `tiktoken` file. Install it with \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install tiktoken`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1456\u001b[0m     )\n\u001b[0;32m   1458\u001b[0m bpe_ranks \u001b[38;5;241m=\u001b[39m load_tiktoken_bpe(tiktoken_url)\n",
      "\u001b[1;31mValueError\u001b[0m: `tiktoken` is required to read a `tiktoken` file. Install it with `pip install tiktoken`.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\wubeshet.abera\\Projects\\Ten-Academy\\Telegram-EthioMart-Entity-Extraction\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2450\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2449\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2450\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2451\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n",
      "File \u001b[1;32mc:\\Users\\wubeshet.abera\\Projects\\Ten-Academy\\Telegram-EthioMart-Entity-Extraction\\venv\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\tokenization_xlm_roberta_fast.py:108\u001b[0m, in \u001b[0;36mXLMRobertaTokenizerFast.__init__\u001b[1;34m(self, vocab_file, tokenizer_file, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m mask_token \u001b[38;5;241m=\u001b[39m AddedToken(mask_token, lstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, rstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mask_token, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m mask_token\n\u001b[1;32m--> 108\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcls_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcls_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43munk_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_file \u001b[38;5;241m=\u001b[39m vocab_file\n",
      "File \u001b[1;32mc:\\Users\\wubeshet.abera\\Projects\\Ten-Academy\\Telegram-EthioMart-Entity-Extraction\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_fast.py:138\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_special_tokens \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditional_special_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n\u001b[1;32m--> 138\u001b[0m fast_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_slow_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tiktoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m slow_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wubeshet.abera\\Projects\\Ten-Academy\\Telegram-EthioMart-Entity-Extraction\\venv\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1594\u001b[0m, in \u001b[0;36mconvert_slow_tokenizer\u001b[1;34m(transformer_tokenizer, from_tiktoken)\u001b[0m\n\u001b[0;32m   1593\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m-> 1594\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1595\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting from Tiktoken failed, if a converter for SentencePiece is available, provide a model path \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1596\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith a SentencePiece tokenizer.model file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1597\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently available slow->fast convertors: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(SLOW_TO_FAST_CONVERTERS\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1598\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Converting from Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast convertors: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForTokenClassification\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m----> 3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmasakhane/afroxlmr-large-ner-masakhaner-1.0_2.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForTokenClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmasakhane/afroxlmr-large-ner-masakhaner-1.0_2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m nlp \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n",
      "File \u001b[1;32mc:\\Users\\wubeshet.abera\\Projects\\Ten-Academy\\Telegram-EthioMart-Entity-Extraction\\venv\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:907\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    905\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    906\u001b[0m         )\n\u001b[1;32m--> 907\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[0;32m    910\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[0;32m    911\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[1;32mc:\\Users\\wubeshet.abera\\Projects\\Ten-Academy\\Telegram-EthioMart-Entity-Extraction\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2216\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2213\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2214\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2219\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2220\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2224\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2225\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2227\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2228\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wubeshet.abera\\Projects\\Ten-Academy\\Telegram-EthioMart-Entity-Extraction\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2451\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2449\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2450\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39minit_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs)\n\u001b[1;32m-> 2451\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43mimport_protobuf_decode_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   2452\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m   2453\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2454\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2455\u001b[0m     )\n\u001b[0;32m   2456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wubeshet.abera\\Projects\\Ten-Academy\\Telegram-EthioMart-Entity-Extraction\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:87\u001b[0m, in \u001b[0;36mimport_protobuf_decode_error\u001b[1;34m(error_message)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DecodeError\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 87\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(PROTOBUF_IMPORT_ERROR\u001b[38;5;241m.\u001b[39mformat(error_message))\n",
      "\u001b[1;31mImportError\u001b[0m: \n requires the protobuf library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"masakhane/afroxlmr-large-ner-masakhaner-1.0_2.0\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"masakhane/afroxlmr-large-ner-masakhaner-1.0_2.0\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = df['Message'][10]\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ğŸ’¥Only baby 3in1 double bottle milk warmer,sterilizer,food steamer\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 á‹‹áŒ‹:-3000á‰¥áˆ­âœ…\\n\\nâŒá‹áˆµáŠ• ááˆ¬ áŠá‹ á‹«áˆˆá‹\\n\\nğŸ¢ áŠ á‹µáˆ«áˆ»\\xa0 á‰.1ğŸ‘‰ áˆµáˆª áŠ¤áˆ áˆ²á‰² áˆáˆ\\xa0 áˆáˆˆá‰°áŠ› áá‰… á‰¢áˆ® á‰. SL-05A(áŠ¨ áˆŠáá‰± áŠá‰µ áˆˆ áŠá‰µ)\\n\\nğŸ“á‰.2 ğŸ‘‰áˆˆá‰¡\\xa0 áˆ˜á‹³áˆ…áŠ’á‹“áˆˆáˆ á‰¤á‰°/áŠ­áˆ­áˆµá‰²á‹«áŠ• áŠá‰µ áˆˆáŠá‰µ\\xa0 #á‹›áˆ_áˆáˆ 2áŠ› áá‰… á‰¢áˆ® á‰áŒ¥áˆ­.214\\n\\nğŸ‘áˆˆá‰¡\\xa0á‰…áˆ­áŠ•áŒ«áğŸ“²0973611819\\n\\n\\n\\n\\xa0\\xa0\\xa0\\xa0 ğŸ’§ğŸ’§ğŸ’§ğŸ’§\\n\\n\\n\\xa0\\xa0\\xa0 ğŸ“² 0909522840\\n\\xa0\\xa0\\xa0 ğŸ“² 0923350054\\n\\nğŸ”–\\nğŸ’¬\\xa0 á‰ Telegram áˆˆáˆ›á‹˜á‹ â¤µï¸ á‹­áŒ á‰€áˆ™\\n@shager_onlinestore\\n\\xa0 \\náˆˆá‰°áŒ¨áˆ›áˆª áˆ›á‰¥áˆ«áˆªá‹« á‹¨á‰´áˆŒáŒáˆ«áˆ áŒˆáƒá‰½áŠ•â¤µï¸\\nhttps://t.me/Shageronlinestore'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Message'][10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('clean_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if a string contains Amharic characters\n",
    "def is_amharic(message):\n",
    "    return bool(re.search(r'[\\u1200-\\u137F]', message))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to classify messages\n",
    "def classify_message(message):\n",
    "    if pd.isna(message):  # Check for NaN or None\n",
    "        return 'uncategorized'\n",
    "    \n",
    "    if is_amharic(message):\n",
    "        for category, keywords in categories.items():\n",
    "            if any(keyword in message for keyword in keywords):\n",
    "                return category\n",
    "    else:\n",
    "        for category, keywords in categories.items():\n",
    "            if any(keyword in message.lower() for keyword in keywords):\n",
    "                return category\n",
    "    return 'uncategorized'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Message   Category\n",
      "5     ğŸ’¥3pcs silicon brush spatulas\\n\\nâš¡áŠ¥áˆµáŠ¨ 260Â°c áˆ™á‰€á‰µ...  household\n",
      "6     ğŸ’¥Mandoline Slicer\\n\\nğŸ‘‰ áŒŠá‹œ á‰†áŒ£á‰¢ áˆµáˆ‹á‹­áˆµ áˆ›á‹µáˆ¨áŒŠá‹« \\nğŸ‘‰Â  ...  household\n",
      "7     ğŸ’¥Table Desk Edge Guard Strip\\nÂ Â Â Â Â Â  ğŸ’¯ High Qu...        men\n",
      "8     ğŸ’¥Table Desk Edge Guard Strip\\nÂ Â Â Â Â Â  ğŸ’¯ High Qu...        men\n",
      "10    ğŸ’¥Only baby 3in1 double bottle milk warmer,ster...       kids\n",
      "...                                                 ...        ...\n",
      "5009  ğŸ¯ Kitchen Sticker\\n\\náˆˆáŠªá‰½áŠ•á‹ á‹á‰ á‰µ áŠ¥áŒ…áŒ á‰°áˆ˜áˆ«áŒ­ \\nğŸ”°á‹áˆ€ ...  household\n",
      "5010  ğŸ¯ 3in1 One Step Hair Dryer & Styler \\n\\nğŸ‘‰ áŠ¨áˆ­áˆ ...      women\n",
      "5011  âœ… Home GYM - X5 slimming vibrator \\n\\nğŸ“¢ğŸ“¢ğŸ“¢ á‰³áˆ‹á‰… ...      sport\n",
      "5012  áˆˆáŒ¤áŠ“á‰½áŠ•-Health & Personal Care\\n\\nğŸ“FingerTip Pul...       kids\n",
      "5013  #Finger_tip_pulse_oximeter\\n       #á‰ á‰°áˆ˜áŒ£áŒ£áŠ_á‹‹áŒ‹\\...     health\n",
      "\n",
      "[3166 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6692/2163786579.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Category'] = df['Message'].apply(classify_message)\n"
     ]
    }
   ],
   "source": [
    "# Apply classification to the Message column\n",
    "df['Category'] = df['Message'].apply(classify_message)\n",
    "\n",
    "# Display the updated DataFrame with categories\n",
    "print(df[['Message', 'Category']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category\n",
      "uncategorized    1337\n",
      "household         821\n",
      "kids              473\n",
      "men               174\n",
      "groceries         152\n",
      "health             70\n",
      "women              50\n",
      "sport              45\n",
      "accessories        44\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display counts of unique values in the Category column\n",
    "category_counts = df['Category'].value_counts()\n",
    "print(category_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel Title</th>\n",
       "      <th>Channel Username</th>\n",
       "      <th>ID</th>\n",
       "      <th>Message</th>\n",
       "      <th>Date</th>\n",
       "      <th>Media Path</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sheger online-store</td>\n",
       "      <td>@Shageronlinestore</td>\n",
       "      <td>5328</td>\n",
       "      <td>ğŸ’¥3pcs silicon brush spatulas\\n\\nâš¡áŠ¥áˆµáŠ¨ 260Â°c áˆ™á‰€á‰µ...</td>\n",
       "      <td>2024-09-20 11:50:02+00:00</td>\n",
       "      <td>photos/@Shageronlinestore_5328.jpg</td>\n",
       "      <td>household</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sheger online-store</td>\n",
       "      <td>@Shageronlinestore</td>\n",
       "      <td>5327</td>\n",
       "      <td>ğŸ’¥Mandoline Slicer\\n\\nğŸ‘‰ áŒŠá‹œ á‰†áŒ£á‰¢ áˆµáˆ‹á‹­áˆµ áˆ›á‹µáˆ¨áŒŠá‹« \\nğŸ‘‰Â  ...</td>\n",
       "      <td>2024-09-20 08:11:40+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>household</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sheger online-store</td>\n",
       "      <td>@Shageronlinestore</td>\n",
       "      <td>5326</td>\n",
       "      <td>ğŸ’¥Table Desk Edge Guard Strip\\nÂ Â Â Â Â Â  ğŸ’¯ High Qu...</td>\n",
       "      <td>2024-09-20 05:23:18+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>men</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sheger online-store</td>\n",
       "      <td>@Shageronlinestore</td>\n",
       "      <td>5325</td>\n",
       "      <td>ğŸ’¥Table Desk Edge Guard Strip\\nÂ Â Â Â Â Â  ğŸ’¯ High Qu...</td>\n",
       "      <td>2024-09-20 05:21:14+00:00</td>\n",
       "      <td>photos/@Shageronlinestore_5325.jpg</td>\n",
       "      <td>men</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sheger online-store</td>\n",
       "      <td>@Shageronlinestore</td>\n",
       "      <td>5323</td>\n",
       "      <td>ğŸ’¥Only baby 3in1 double bottle milk warmer,ster...</td>\n",
       "      <td>2024-09-19 13:54:46+00:00</td>\n",
       "      <td>photos/@Shageronlinestore_5323.jpg</td>\n",
       "      <td>kids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>Sheger online-store</td>\n",
       "      <td>@Shageronlinestore</td>\n",
       "      <td>5142</td>\n",
       "      <td>ğŸŒŸWINNING STARÂ® 2in1 MULTIFUNCTIONAL BLENDER\\n\\...</td>\n",
       "      <td>2024-08-29 09:12:06+00:00</td>\n",
       "      <td>photos/@Shageronlinestore_5142.jpg</td>\n",
       "      <td>kids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>Sheger online-store</td>\n",
       "      <td>@Shageronlinestore</td>\n",
       "      <td>5141</td>\n",
       "      <td>ğŸ¥‚3.6L Glass dispenser jar with Bamboo stand\\n\\...</td>\n",
       "      <td>2024-08-29 06:02:13+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>household</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>Sheger online-store</td>\n",
       "      <td>@Shageronlinestore</td>\n",
       "      <td>5140</td>\n",
       "      <td>ğŸ¥‚3.6L Glass dispenser jar with Bamboo stand\\n\\...</td>\n",
       "      <td>2024-08-29 06:01:02+00:00</td>\n",
       "      <td>photos/@Shageronlinestore_5140.jpg</td>\n",
       "      <td>household</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>Sheger online-store</td>\n",
       "      <td>@Shageronlinestore</td>\n",
       "      <td>5139</td>\n",
       "      <td>ğŸ’¥44CM HAOCHUÂ® CERAMIC PIZZA PAN\\n\\nâš¡ï¸áˆˆá‰¤á‰µáŠ“ áˆˆáˆ¬áˆµá‰¶...</td>\n",
       "      <td>2024-08-28 18:38:44+00:00</td>\n",
       "      <td>photos/@Shageronlinestore_5139.jpg</td>\n",
       "      <td>household</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>Sheger online-store</td>\n",
       "      <td>@Shageronlinestore</td>\n",
       "      <td>5138</td>\n",
       "      <td>ğŸ’¥Silver CrestÂ®Â touch technology electric stove...</td>\n",
       "      <td>2024-08-28 14:11:54+00:00</td>\n",
       "      <td>photos/@Shageronlinestore_5138.jpg</td>\n",
       "      <td>household</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Channel Title    Channel Username    ID  \\\n",
       "5    Sheger online-store  @Shageronlinestore  5328   \n",
       "6    Sheger online-store  @Shageronlinestore  5327   \n",
       "7    Sheger online-store  @Shageronlinestore  5326   \n",
       "8    Sheger online-store  @Shageronlinestore  5325   \n",
       "10   Sheger online-store  @Shageronlinestore  5323   \n",
       "..                   ...                 ...   ...   \n",
       "181  Sheger online-store  @Shageronlinestore  5142   \n",
       "182  Sheger online-store  @Shageronlinestore  5141   \n",
       "183  Sheger online-store  @Shageronlinestore  5140   \n",
       "184  Sheger online-store  @Shageronlinestore  5139   \n",
       "185  Sheger online-store  @Shageronlinestore  5138   \n",
       "\n",
       "                                               Message  \\\n",
       "5    ğŸ’¥3pcs silicon brush spatulas\\n\\nâš¡áŠ¥áˆµáŠ¨ 260Â°c áˆ™á‰€á‰µ...   \n",
       "6    ğŸ’¥Mandoline Slicer\\n\\nğŸ‘‰ áŒŠá‹œ á‰†áŒ£á‰¢ áˆµáˆ‹á‹­áˆµ áˆ›á‹µáˆ¨áŒŠá‹« \\nğŸ‘‰Â  ...   \n",
       "7    ğŸ’¥Table Desk Edge Guard Strip\\nÂ Â Â Â Â Â  ğŸ’¯ High Qu...   \n",
       "8    ğŸ’¥Table Desk Edge Guard Strip\\nÂ Â Â Â Â Â  ğŸ’¯ High Qu...   \n",
       "10   ğŸ’¥Only baby 3in1 double bottle milk warmer,ster...   \n",
       "..                                                 ...   \n",
       "181  ğŸŒŸWINNING STARÂ® 2in1 MULTIFUNCTIONAL BLENDER\\n\\...   \n",
       "182  ğŸ¥‚3.6L Glass dispenser jar with Bamboo stand\\n\\...   \n",
       "183  ğŸ¥‚3.6L Glass dispenser jar with Bamboo stand\\n\\...   \n",
       "184  ğŸ’¥44CM HAOCHUÂ® CERAMIC PIZZA PAN\\n\\nâš¡ï¸áˆˆá‰¤á‰µáŠ“ áˆˆáˆ¬áˆµá‰¶...   \n",
       "185  ğŸ’¥Silver CrestÂ®Â touch technology electric stove...   \n",
       "\n",
       "                          Date                          Media Path   Category  \n",
       "5    2024-09-20 11:50:02+00:00  photos/@Shageronlinestore_5328.jpg  household  \n",
       "6    2024-09-20 08:11:40+00:00                                 NaN  household  \n",
       "7    2024-09-20 05:23:18+00:00                                 NaN        men  \n",
       "8    2024-09-20 05:21:14+00:00  photos/@Shageronlinestore_5325.jpg        men  \n",
       "10   2024-09-19 13:54:46+00:00  photos/@Shageronlinestore_5323.jpg       kids  \n",
       "..                         ...                                 ...        ...  \n",
       "181  2024-08-29 09:12:06+00:00  photos/@Shageronlinestore_5142.jpg       kids  \n",
       "182  2024-08-29 06:02:13+00:00                                 NaN  household  \n",
       "183  2024-08-29 06:01:02+00:00  photos/@Shageronlinestore_5140.jpg  household  \n",
       "184  2024-08-28 18:38:44+00:00  photos/@Shageronlinestore_5139.jpg  household  \n",
       "185  2024-08-28 14:11:54+00:00  photos/@Shageronlinestore_5138.jpg  household  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from wordcloud import WordCloud\n",
    "# import matplotlib.pyplot as plt\n",
    "# # Filter for uncategorized items\n",
    "# uncategorized_items = df[df['Category'] == 'uncategorized']\n",
    "\n",
    "# # Combine all messages into a single string\n",
    "# text = ' '.join(uncategorized_items['Message'])\n",
    "\n",
    "# # Generate the word cloud\n",
    "# wordcloud = WordCloud(width=800, height=400, background_color='white', \n",
    "#                       colormap='viridis', max_words=200).generate(text)\n",
    "\n",
    "# # Display the word cloud\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.imshow(wordcloud, interpolation='bilinear')\n",
    "# plt.axis('off')  # Turn off the axis\n",
    "# plt.title('Word Cloud of Messages for Uncategorized Items')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel Title</th>\n",
       "      <th>Channel Username</th>\n",
       "      <th>ID</th>\n",
       "      <th>Message</th>\n",
       "      <th>Date</th>\n",
       "      <th>Media Path</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Sheger online-store</td>\n",
       "      <td>@Shageronlinestore</td>\n",
       "      <td>5284</td>\n",
       "      <td>â­ï¸ğŸ’«áŠ¥áŠ•áŠ³áŠ• áˆˆáˆ˜á‹áˆŠá‹µ á‰ á‹“áˆ á‰ áˆ°áˆ‹áˆ áŠ á‹°áˆ¨áˆ°á‹\\n\\náˆ˜áˆáŠ«áˆ á‰ á‹“áˆ</td>\n",
       "      <td>2024-09-15 08:53:29+00:00</td>\n",
       "      <td>photos/@Shageronlinestore_5284.jpg</td>\n",
       "      <td>uncategorized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Sheger online-store</td>\n",
       "      <td>@Shageronlinestore</td>\n",
       "      <td>5283</td>\n",
       "      <td>ğŸ’¥ á‹á‹µ á‹°áŠ•á‰ áŠá‰»á‰½áŠ• áˆ±á‰ƒá‰½áŠ• á‹›áˆ¬  áŠ¥áˆá‹µ áŠ¨5:00-9:00 áˆµá‹“á‰µ áŠ­áá‰µ áˆ˜...</td>\n",
       "      <td>2024-09-15 07:06:34+00:00</td>\n",
       "      <td>photos/@Shageronlinestore_5283.jpg</td>\n",
       "      <td>uncategorized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Sheger online-store</td>\n",
       "      <td>@Shageronlinestore</td>\n",
       "      <td>5256</td>\n",
       "      <td>ğŸ’¥á‹áˆµáŠ• ááˆ¬ á‹¨á‰€áˆ©áŠ• á‹•á‰ƒá‹á‰½\\nğŸŒ¼ğŸŒ¼.................ğŸŒ¼ğŸŒ¼\\n\\nâœ¨áˆ...</td>\n",
       "      <td>2024-09-10 05:31:03+00:00</td>\n",
       "      <td>photos/@Shageronlinestore_5256.jpg</td>\n",
       "      <td>uncategorized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Sheger online-store</td>\n",
       "      <td>@Shageronlinestore</td>\n",
       "      <td>5237</td>\n",
       "      <td>ğŸŒ¼ğŸŒ¼........................ğŸŒ¼ğŸŒ¼\\nğŸ’¥á‰ áˆ¨áá‰µ á‰€áŠ•á‹ áˆ±á‰… áˆ‹á‹­ ...</td>\n",
       "      <td>2024-09-07 16:58:25+00:00</td>\n",
       "      <td>photos/@Shageronlinestore_5237.jpg</td>\n",
       "      <td>uncategorized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>Sheger online-store</td>\n",
       "      <td>@Shageronlinestore</td>\n",
       "      <td>5206</td>\n",
       "      <td>ğŸŒ¼ğŸŒ¼........................ğŸŒ¼ğŸŒ¼\\nğŸ’¥EthereumÂ® Washi...</td>\n",
       "      <td>2024-09-05 08:07:01+00:00</td>\n",
       "      <td>photos/@Shageronlinestore_5206.jpg</td>\n",
       "      <td>uncategorized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>Sheger online-store</td>\n",
       "      <td>@Shageronlinestore</td>\n",
       "      <td>4654</td>\n",
       "      <td>ğŸ’¥Painless hair eraser\\n\\n       á‹‹áŒ‹-500á‰¥áˆ­\\n\\nğŸ¢ ...</td>\n",
       "      <td>2024-07-04 07:53:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>uncategorized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>Sheger online-store</td>\n",
       "      <td>@Shageronlinestore</td>\n",
       "      <td>4649</td>\n",
       "      <td>ğŸ‘¶Kids knitted winter warm hat with scarfğŸ§£\\n\\nâš¡...</td>\n",
       "      <td>2024-07-03 14:43:54+00:00</td>\n",
       "      <td>photos/@Shageronlinestore_4649.jpg</td>\n",
       "      <td>uncategorized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>Sheger online-store</td>\n",
       "      <td>@Shageronlinestore</td>\n",
       "      <td>4640</td>\n",
       "      <td>â‡ï¸ Wall Mounted Phone Holder Charging Stand La...</td>\n",
       "      <td>2024-07-03 07:43:48+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>uncategorized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>Sheger online-store</td>\n",
       "      <td>@Shageronlinestore</td>\n",
       "      <td>4639</td>\n",
       "      <td>â‡ï¸ Wall Mounted Phone Holder Charging Stand La...</td>\n",
       "      <td>2024-07-03 07:38:21+00:00</td>\n",
       "      <td>photos/@Shageronlinestore_4639.jpg</td>\n",
       "      <td>uncategorized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>Sheger online-store</td>\n",
       "      <td>@Shageronlinestore</td>\n",
       "      <td>4638</td>\n",
       "      <td>ğŸ’¥Philips Easy Speed Iron\\n\\nğŸŒŸEasy to use\\nğŸŒŸWat...</td>\n",
       "      <td>2024-07-02 19:24:15+00:00</td>\n",
       "      <td>photos/@Shageronlinestore_4638.jpg</td>\n",
       "      <td>uncategorized</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Channel Title    Channel Username    ID  \\\n",
       "47   Sheger online-store  @Shageronlinestore  5284   \n",
       "48   Sheger online-store  @Shageronlinestore  5283   \n",
       "72   Sheger online-store  @Shageronlinestore  5256   \n",
       "91   Sheger online-store  @Shageronlinestore  5237   \n",
       "120  Sheger online-store  @Shageronlinestore  5206   \n",
       "..                   ...                 ...   ...   \n",
       "628  Sheger online-store  @Shageronlinestore  4654   \n",
       "633  Sheger online-store  @Shageronlinestore  4649   \n",
       "642  Sheger online-store  @Shageronlinestore  4640   \n",
       "643  Sheger online-store  @Shageronlinestore  4639   \n",
       "644  Sheger online-store  @Shageronlinestore  4638   \n",
       "\n",
       "                                               Message  \\\n",
       "47            â­ï¸ğŸ’«áŠ¥áŠ•áŠ³áŠ• áˆˆáˆ˜á‹áˆŠá‹µ á‰ á‹“áˆ á‰ áˆ°áˆ‹áˆ áŠ á‹°áˆ¨áˆ°á‹\\n\\náˆ˜áˆáŠ«áˆ á‰ á‹“áˆ   \n",
       "48   ğŸ’¥ á‹á‹µ á‹°áŠ•á‰ áŠá‰»á‰½áŠ• áˆ±á‰ƒá‰½áŠ• á‹›áˆ¬  áŠ¥áˆá‹µ áŠ¨5:00-9:00 áˆµá‹“á‰µ áŠ­áá‰µ áˆ˜...   \n",
       "72   ğŸ’¥á‹áˆµáŠ• ááˆ¬ á‹¨á‰€áˆ©áŠ• á‹•á‰ƒá‹á‰½\\nğŸŒ¼ğŸŒ¼.................ğŸŒ¼ğŸŒ¼\\n\\nâœ¨áˆ...   \n",
       "91   ğŸŒ¼ğŸŒ¼........................ğŸŒ¼ğŸŒ¼\\nğŸ’¥á‰ áˆ¨áá‰µ á‰€áŠ•á‹ áˆ±á‰… áˆ‹á‹­ ...   \n",
       "120  ğŸŒ¼ğŸŒ¼........................ğŸŒ¼ğŸŒ¼\\nğŸ’¥EthereumÂ® Washi...   \n",
       "..                                                 ...   \n",
       "628  ğŸ’¥Painless hair eraser\\n\\n       á‹‹áŒ‹-500á‰¥áˆ­\\n\\nğŸ¢ ...   \n",
       "633  ğŸ‘¶Kids knitted winter warm hat with scarfğŸ§£\\n\\nâš¡...   \n",
       "642  â‡ï¸ Wall Mounted Phone Holder Charging Stand La...   \n",
       "643  â‡ï¸ Wall Mounted Phone Holder Charging Stand La...   \n",
       "644  ğŸ’¥Philips Easy Speed Iron\\n\\nğŸŒŸEasy to use\\nğŸŒŸWat...   \n",
       "\n",
       "                          Date                          Media Path  \\\n",
       "47   2024-09-15 08:53:29+00:00  photos/@Shageronlinestore_5284.jpg   \n",
       "48   2024-09-15 07:06:34+00:00  photos/@Shageronlinestore_5283.jpg   \n",
       "72   2024-09-10 05:31:03+00:00  photos/@Shageronlinestore_5256.jpg   \n",
       "91   2024-09-07 16:58:25+00:00  photos/@Shageronlinestore_5237.jpg   \n",
       "120  2024-09-05 08:07:01+00:00  photos/@Shageronlinestore_5206.jpg   \n",
       "..                         ...                                 ...   \n",
       "628  2024-07-04 07:53:00+00:00                                 NaN   \n",
       "633  2024-07-03 14:43:54+00:00  photos/@Shageronlinestore_4649.jpg   \n",
       "642  2024-07-03 07:43:48+00:00                                 NaN   \n",
       "643  2024-07-03 07:38:21+00:00  photos/@Shageronlinestore_4639.jpg   \n",
       "644  2024-07-02 19:24:15+00:00  photos/@Shageronlinestore_4638.jpg   \n",
       "\n",
       "          Category  \n",
       "47   uncategorized  \n",
       "48   uncategorized  \n",
       "72   uncategorized  \n",
       "91   uncategorized  \n",
       "120  uncategorized  \n",
       "..             ...  \n",
       "628  uncategorized  \n",
       "633  uncategorized  \n",
       "642  uncategorized  \n",
       "643  uncategorized  \n",
       "644  uncategorized  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncategorized_items = df[df['Category'] == 'uncategorized']\n",
    "uncategorized_items.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('labeled_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncategorized_items.to_csv('uncategorized_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
